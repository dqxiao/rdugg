\section{State-Of-the-Art}
\label{sec:state-of-the-art}
\subsection{Distributed Triangle Listing Algorithms}
Triangle listing is a basic operation of the graph analysis. Many research works have been conducted on this problem, which can be classified into three categories: in-memory algorithms, external-memory algorithms and distributed algorithms. Here, we briefly review these works. 

\textbf{In-Memory Algorithm.} 
The majority of previously introduced triangle listing algorithms are the In-Memory processing approaches. Traditionally, they can be further classified as Node-Iterator\cite{Alon_Yuster_Zwick_1997,Batagelj_Mrvar_2001,Schank_2007} and Edge-Iterator ones\cite{
Itai_Rodeh_1978,Chiba_Nishizeki_1985} with the respect to iterator-type. Authors \cite{Itai_Rodeh_1978,Chiba_Nishizeki_1985,Schank_2007}improved the performance of in-memory algorithms by adopting degree-based ordering. 
Matrix multiplication is used to count triangles \cite{Alon_Yuster_Zwick_1997}. However, all these algorithms are inapplicable to massive graphs which do not fit in memory. 

\textbf{External-Memory Algorithms.} In order to handle the massive graph, several external-memory approaches were introduced \cite{H14,Kim_Han_Lee_Park_Yu_2014,GraphChi}. Common idea of these methods is: (1)~Partition the input graph to make each partition fit into main-memory, (2)~Load each partition individually into main-memory and identify all its triangles, and then remove edges which participated in the identified triangle, and (3)~After the whole graph is loaded into memory buffer once, the remaining edges are merged, then repeat former steps until no edges remain. These Algorithms require a lot of disk I/Os to perform the reading and writing of the edges. 
Authors ~\cite{H14,Kim_Han_Lee_Park_Yu_2014} improved the performance by reducing the amount of disk I/Os and exploiting multi-core parallelism. External-Memory Algorithms show great performance in time and space. However, the parallelization of external-memory algorithms is limited. External-memory approaches cannot easily scale up in terms of computing resources and parallelization degree.

\textbf{Distributed Algorithms.} 
Another promising approach to handle triangle listing on large-scale graphs is the distributed computing. Suri {\etal} \cite{Suri_Vassilvitskii_2011} introduced two Map-Reduce adaptions of NodeIterator algorithm and the well-known Graph Partitioning (GP) algorithm to count triangles. The Graph Partitioning algorithm utilizes one universal hash partition function over nodes to distribute edges into overlapped graph partitions, then identifies triangles over all the partitions. Park {\etal} \cite{parkmapreduce2014} further generalized Graph Partitioning algorithm into multiple rounds, significantly increasing the size of the graphs that can be handled on a given system. The authors compare their algorithm with GP algorithm \cite{Suri_Vassilvitskii_2011} across various massive graphs then show that they get speedups ranging from 2 to 5. In this work, we show such large or even larger speedup (from 5 to 10) can also be obtained by reducing the size intermediate result directly via our methods. Teixeira {\etal} \cite{Teixeira_2015} presented Arabesque, one distributed data processing platform for implementing subgraph mining algorithms on the basis of MapReduce framework.  Arabesque automates the process of exploring a very large number of subgraphs, including triangles. However, these MapReduce algorithms must generate a large amount of intermediate data that travel over the network during the shuffle operation, which degrade their performance. Arifuzzaman {\etal} \cite{Patric} introduced an efficient MPI-based distributed memory parallel algorithm (Patric) on the basis of NodeIterator algorithm. The Patric algorithm introduced degree-based sorting preprocessing step for efficient set intersection operation to speed up execution. 
Furthermore, several distributed solutions designed for subgraph mining on large graph were also proposed \cite{Pregel,Shao_2014}. Shao {\etal} introduced the PSgl framework  to iteratively enumerate subgraph instance. Different from other parallel approaches, the PSgl framework completes relies on the graph traversal and avoids the explicit join operation. These distributed memory parallel algorithms achieve impressive performance over large-scale graph mining tasks. These methods distributed the data graph among the worker's memory, thus they are not suitable for processing large-scale graph with small clusters.

\subsection{Graph Anonymization}
Uncertain graph anonymization problem is related to the conventional graph anonymization problem. Several definitions and methods have been proposed to protect users' privacy when publicly releasing graph data. Here, we focus on graph-modification techniques which alter graph's structure and release the entire anonymous network, allowing researchers and third parties to apply all graph-mining process on anonymous data, from local to global knowledge extraction. 

Note that for defining the problem of privacy preserving graph publishing, we need to formulate the following issue: firstly, we need to identify information to be preserved. Secondly, we need to model the background knowledge that an adversary may use to attack the privacy. And thirdly, we need to specify the usage of the published graph data so that an anonymization method can try to retain the utility as much as possible while the privacy information is fully preserved. 

Regarding the privacy information to be preserved in the graph data, three main categories of privacy threats have been identified: 
\begin{enumerate}
    \item {\em Identity disclosure} occurs when the identity of an individual who is associated with a vertex is revealed. It includes sub-categories such as vertex existence, vertex properties, and graph metrics. 
    \item {\em Attribute disclosure} which seeks not necessarily to identify a vertex, but to reveal sensitive labels of the vertex. 
    \item {\em Link disclosure} when the sensitive relationship between two individuals is disclosed. Depending on graph's type, we can refine this category as link relationships, link weight, and sensitive edge labels. 
\end{enumerate}

Identify disclosure often lead to attribute disclosure due to that fact that identity disclosure occurs when an individual is identified within a dataset, whereas attribute disclosure occurs when sensitive information that an individual wished to keep private is identified. 

Determining the knowledge of the adversary is a challenging problem. A variety of adversaries' knowledge has been proposed in conjunction with their attack and a protection method. Attacks on naively anonymized network data have been developed, which can reidentify vertices, disclose edges between vertices. These attacks include matching attacks, which use external knowledge of vertex features~\cite{Liu_Towards_2008,Wu_k_2010,Boldi_Injecting_2012,Zhou_Preserving_2008}; injection attacks which alter the network prior to publication~\cite{Backstrom2011}; and auxiliary network attacks which use publicly available networks as an external information source~\cite{Narayanan2009}. To solve these problems, methods which introduce noise to the original data have been developed in order to hinder the potential process of re-identification. 

\subsubsection{Graph modification techniques}
From a high-level view, there are three general families of graph-modification techniques to mitigate graph data privacy: 

\begin{itemize}
    \item {\em Generalization or clustering-based approaches} which can be essentially regarded as grouping vertices and edges into partitions called super-vertices and super-edges. The details about individuals can be hidden properly, but the graph may be shrunk considerably after anonymization, which may not be desirable for analyzing local structures. 
    \item {\em Edge and vertex modification} approaches first transform the data by edges or vertices modification (adding and/or deleting) and then release the perturbed data. The data is thus made available for unconstrained analysis with existing graph mining techniques. 
    \item {\em Uncertain graphs} are approaches based on adding or removing edges ``partially" by assigning a probability to each edge in the anonymous network. Instead of creating or deleting edges, the set of all possible edges is considered and a probability is assigned to each edge. 
\end{itemize}

All mentioned methods first transform the data into different types of graph's modifications and then release the perturbed data. The data is thus made available for unconstrained analysis. On the contrary, there is ``privacy preserving graph mining" methods, which do not release data, but only the output of an analysis task. For instance, differential privacy~\cite{Jorgensen2016} is a well-known privacy preserving graph mining method. In our work, we do not consider this method for anonymizing uncertain graphs, since they do not allow us to release the entire network, which provides the widest range of applications for data mining and knowledge extraction. 

\hspace{-2em}\textbf{$\bullet$~Generalization approaches}\\
Generalization approaches can be essentially regarded as grouping vertices and edges into partitions called super-vertices and super-edges. The details about individuals can be hidden properly, but the graph may be shrunk considerably after anonymization, which may be not desirable for analyzing local structures. All methods developed, therefore, need the whole graph to be applied to. Consequently, they are not able to deal with the streaming graph data. Here, we remind the reader new methods can be developed using this core idea to generate anonymous graph dataset. The first approach of this category was proposed by Hay {\etal}~\cite{Hay_Anonymizing_2007}. It uses the size of the partition to ensure node anonymity. After grouping, each super-vertex represents at least $k$ nodes and each super-edge represents all the nodes between nodes in two super-vertices. Only the edge density is published for each partition, so it will be hard to distinguish between individuals in a partition. A similar idea was applied for the complex network, {\ie}, labeled network~\cite{Bhagat_Class_2009}.The clustering problem is known to be NP-hard. Researchers ever present different methods for optimization. For instance, Sihag {\etal} chose the genetic algorithm to optimize this NP-hard problem. It does achieve a better result in terms of information loss. Unfortunately, this method does not seem scalable for large networks. 


\hspace{-2em}\textbf{$\bullet$~Edge and vertex modification approaches}\\
Edge and vertex modification approaches anonymize a graph by modifying edges or vertices in the graph. These modifications can be made at random (referred to as {\em randomization, random perturbation}). {\em random perturbation} techniques are generally the simplest and present the lowest complexity. Thus, they are able to deal with large networks. The first method was proposed by Hay {\etal}, called {\em Random perturbation} which anonymizes unlabelled graphs using Rand add/del strategy, {\ie}, randomly removing $p$ edges and then randomly add $p$ fake edges without change the set of vertices and the total number of edges. On this basis, Ying and Wu~\cite{Ying2009,Ying_Randomizing_2008} developed two algorithms designed to preserve spectral characteristics of the original graph called {\em Spctr Add/Del} and {\em Spctr Switch}. Following the path, Stokes and Torra states an appropriate selection of the eigenvalues in the spectral method  can perturbation the graph while keeping its most significative edges. The generic strategy which aims to preserve the most important edges in the network trying to maximize utility while achieving the desired privacy level. Generally, such utility-aware methods achieve lower information loss, but at a cost of increasing complexity. Another improved variation of {\em Random perturbation} was proposed by Ying {\etal}~\cite{Ying2009}, called {\em Blockwise Random Add/Del}. This method divides the graph into blocks according to the degree sequence and implements edge modifications on the vertices at high risk of re-identification, not at random over the entire set of vertices. However, the ever-mentioned random perturbation techniques do not offer privacy guarantee.

The modification can be performed in order to fulfill some desired constraints (referred to as {\em constrained perturbation methods}). Among them, the $k$-anonymity model is the most well-known privacy notation which imported from relational data anonymization. The $k$-anonymity model indicates that an attacker can not distinguish between $k$ records although he manages to find a group of quasi-identifiers. Therefore, the attacker can not re-identify an individual with a probability greater than $\frac{1}{k}$. The concept can be used as quasi-identifier to extend $k$-anonymity on the graph data such as $k$-degree anonymity. 

Constrained graph modification based on modifying the graph structure (by edge modifications) to ensure all the vertices satisfy $k$-anonymity. The first method was proposed by Liu and Terzi~\cite{Liu_Towards_2008} which based on integer linear programming and edge switch  to construct a new anonymous graph which is $k$-degree anonymous. Hartung {\etal}~\cite{Hartung_Theory_2015} showed $k$-degree anonymity becomes NP-hard on graphs with H-index three, which is a quite common case for large networks. Different kinds of heuristics were proposed to improve over Liu and Terzi's work in terms of speed and scalability~\cite{Nagle_EWNI_2012,}. For instance, Nagle {\etal}~\cite{Nagle_EWNI_2012} proposed a local anonymization algorithm based on $k$-degree anonymity that focuses on obscuring structurally important vertices that are not well anonymized, thereby reducing the cost of the overall anonymization procedure. However, results are similar to Liu and Terzi's algorithm in terms of information loss. Namely, they suffer from the high utility low bound. 

\hspace{-2em}\textbf{$\bullet$~Uncertain graphs}\\
Rather than anonymization graphs by generalized them or adding/removing edges to satisfy privacy parameter, recent methods have explored the semantics of uncertain graphs to achieve privacy protection. The first approach was proposed by Boldi {\etal}~\cite{Boldi_Injecting_2012}. It is based on injecting uncertainty in deterministic graphs and publishing the resulting uncertain graphs. The authors notice that from a probabilistic perspective, adding a non-existing edge corresponds to changing its existence probability from 1 to 0 vice versa. In their method, instead of considering only binary edge probabilities, they allow probabilities to take any value in the range $[0,1]$. From the perspective of graph modification, they provide more gained way ``partial Add/Del Edge" to transform the input graph to the anonymous one thereby reduce the information loss in the anonymization procedure. However, the specific method ignores several opportunities for further reducing information loss in the anonymization procedure. Nguyen {\etal}~\cite{Nguyen_Anonymizing_2015} proposed a generalized obfuscation model based on uncertain adjacency matrices that keep expected node degrees equals to those in the original graph, and a generic framework for privacy and utility quantification of anonymization methods. The same authors present another method based on maximum variance to achieve better trade-off privacy and data utility (referred to as {\em MaxVar}). In particular, they transform the optimization problem into independent quadratic optimization problems by dividing the large input graph into subgraphs. From the view of graph modification, they provide more subtle way ``partial Switch Edge" for anonymizing the input graph thereby achieve the better trade-off between privacy and utility. However, {\em MaxVar} fails to provide meaning privacy guarantee for user tunable purpose. 
What's more, these two methods assumes each edge modification has the equal impact over the graph. As ever shown in ever-discussed vertex and edge modification techniques, it is not always the case, especially in large networks. 
